#just trying to see how a direct call to OpenAI looks like---
llm = ChatOpenAI(openai_api_key="YOUR OPEN AI KEY")
llm.invoke("be descriptive about a drone specification")

import chromadb
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,)
from langchain_community.vectorstores import Chroma
import chromadb.utils.embedding_functions as embedding_functions
from chromadb.utils import embedding_functions
from langchain import hub
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# We will use an embeddings model that converts text input into numerical values (vectors) that can be used for various tasks like
# semantic search. We will store our word vectors in a database called Chroma. The conversation process is made possible through a 
# function that maps one "space" to another larger or more dense space...so we are mapping a word e.g. drone to a value like "45.6, 46.7,...".
# Here we will use the embedding function "all-MiniLM-L6-V2". 
embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# We will create a database called myDB5 that will store all the vectors for each word that is part of a local repository of documents. 
#In our case, data-sheets---one for a "drone" and the other for a "smart watch"

path = "myDB5"
client = chromadb.PersistentClient(path)

#We will load the drone file" into the database" 
loader = PyPDFLoader("FlybyNight Drone Datasheet.pdf") 
documents = loader.load()

# Now we will split our document into individual characters 
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# We will load the document into our database so we can use it later for searches. However, we will store the vector values for each word 
# in our document. The word "collection" references an organizational structure in the database to segregate differnet types of documents. 
# Our database is called my_collection. 
collection = client.get_or_create_collection(name="my_collection") 
db = Chroma.from_documents(documents = docs, embedding = embedding_function, ids=None, collection_name="my_collection", persist_directory= path)

# similarity search
question = "be descriptive about a drone datasheet"

docs = db.similarity_search_with_score(question,k=1)
 
for result in docs:
    print("\n")
    print(result[1])
    print(result[0].page_content)

collection = client.get_or_create_collection(name="my_collection") 
db = Chroma.from_documents(documents = docs, embedding = embedding_function, ids=None, collection_name="my_collection", persist_directory= path)

#A vector store retriever uses a vector store to retrieve documents. It is a lightweight wrapper around the 
#vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, 
#like similarity search and MMR, to query the texts in the vector store.
retriever = db.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
#output_parser = StrOutputParser()

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
    
retrieval_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
)

retrieval_chain.invoke("be descriptive about a drone specification")

                       #We will load the Smart Watch Data Sheeet into the database as words that are represted as vectors
loader = TextLoader("./smartwatch_datasheet.txt")
documents = loader.load()

# Now we will split our document into individual characters 
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# We will load the document into our database so we can use it later for searches. However, we will store the vector values for each word 
# in our document. The word "collection" references an organizational structure in the database to segregate differnet types of documents. 
# Our database is called my_collection. 

collection = client.get_or_create_collection(name="my_collection") 
db = Chroma.from_documents(documents = docs, embedding = embedding_function, ids=None, collection_name="my_collection", persist_directory= path)

                       # similarity search
question = "be descriptive about a smart watch specification"

docs = db.similarity_search_with_score(question,k=1)

for result in docs:
    print("\n")
    print(result[1])
    print(result[0].page_content)

                       #A vector store retriever uses a vector store to retrieve documents. It is a lightweight wrapper around the 
#vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, 
#like similarity search and MMR, to query the texts in the vector store.
retriever = db.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
#output_parser = StrOutputParser()

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
    
retrieval_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
)

retrieval_chain.invoke("be descriptive about a smart watch specification")
